# -*- coding: utf-8 -*-
"""B19EE046_DAI_A3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zB9daByMnhURvqY8jROFZ3fc2MPLEknf
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install pytorch_lightning

import os
import torch
from torch.utils.data import Dataset, random_split, DataLoader, ConcatDataset, IterableDataset
import pytorch_lightning as pl
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
import numpy as np
import matplotlib.pyplot as plt
from torchvision import models
import PIL
from tqdm import tqdm
import cv2
import copy
from sklearn.metrics import classification_report

def transform_mnist(img):
  img = img.resize((32,32))
  tensor = transforms.ToTensor()(img)
  tensor = torch.concatenate([tensor,tensor,tensor],dim=0)
  return tensor

def transform_cmnist(img):
  img = img.resize((32,32))
  tensor = transforms.ToTensor()(img)
  tensor = torch.concatenate([tensor,tensor,tensor],dim=0) + torch.rand(size=(3,1,1))
  return tensor

def transform_svhn(img):
  img = img.resize((32,32))
  tensor = transforms.ToTensor()(img)
  return tensor

def confusion_matrix(pred,true):
  conf_matrix = np.zeros((10,10),dtype='uint8')
  for i in range(len(pred)):
    conf_matrix[pred[i]][true[i]]+=1
  print(conf_matrix)

def accuracy(pred,true):
  class_vector = np.zeros((1,10))
  total_vector = np.zeros((1,10))
  total_correct = 0
  for i in range(len(pred)):
    if pred[i]==true[i]:
      total_correct+=1
      class_vector[0][true[i]]+=1
    total_vector[0][true[i]]+=1
  print("\nOverall Accuracy = ", 100*total_correct/len(pred), "%")
  print("\nClass-Wise Accuracies = ", 100*class_vector/total_vector)
  print(classification_report(pred,true))

def detect(test_loader, model_class):
  pred=[]
  true=[]
  model_class.cuda()
  for i,j in tqdm(test_loader):
    i,j = i.cuda(), j.cuda()
    prob = model_class(i)
    out = prob.max(1, keepdim=True)[1]
    pred.append(out.detach().cpu().item())
    true.append(j.detach().cpu().item())
  print("Confusion Matrix:\n")
  confusion_matrix(pred, true)
  accuracy(pred, true)
  del pred, true

def federated_learning(datasets, num_epochs, name):
  num_clients = len(datasets)
  client_models=[]
  for i, dataset in enumerate(datasets):
    train_loader, test_loader = dataset
    client_model = Model(name,10)
    trainer = pl.Trainer(accelerator='gpu', max_epochs=num_epochs, default_root_dir='/content/drive/MyDrive/DAI_Assignment_3/'+str(name)+'_'+str(i+1), benchmark=True)
    trainer.fit(client_model, train_loader)
    client_models.append(client_model)
    del client_model, trainer
  model = Model(name,10)
  for name, param in model.named_parameters():
    param.data = torch.zeros_like(param.data)
    for client_model in client_models:
      param.data += client_model.state_dict()[name] / num_clients
  return model, client_models

def model_return(ckpt_path, model_obj):
  ckpt = torch.load(ckpt_path)
  model_obj.load_state_dict(ckpt['state_dict'])
  return model_obj.eval().cuda()

def random_pick_mnist(num_per_class, dataset, b_size):
  classes = list(range(10))
  indices = []
  for c in classes:
    indices.extend(torch.where(dataset.targets == c)[0][:num_per_class])
  subset_set = torch.utils.data.Subset(dataset, indices)
  loader = DataLoader(subset_set, batch_size=b_size, shuffle=True)
  return loader

def random_pick_svhn(num_per_class, dataset, b_size):
  classes = list(range(10))
  indices = []
  for c in classes:
    indices.extend(np.where(dataset.labels == c)[0][:num_per_class])
  subset_set = torch.utils.data.Subset(dataset, indices)
  loader = DataLoader(subset_set, batch_size=b_size, shuffle=True)
  return loader

def server_aggregate(global_model, client_models):
  global_dict = global_model.state_dict()
  for k in global_dict.keys():
    global_dict[k] = torch.stack([client_models[i].state_dict()[k].float() for i in range(len(client_models))], 0).mean(0)
  global_model.load_state_dict(global_dict)
  for model in client_models:
    model.load_state_dict(global_model.state_dict())
  return global_model

class Model(pl.LightningModule):

  def __init__(self, model_name, num_classes):
    super().__init__()
    self.model_name = model_name
    if self.model_name=='resnet_34':
      self.model = models.resnet34(weights=None)
      self.features = self.model.fc.in_features 
      self.model.fc = nn.Linear(self.features,num_classes)
    if self.model_name=='convnext_tiny':
      self.model = models.convnext_tiny(weights=None)
      self.features = self.model.classifier[-1].in_features 
      self.model.classifier[-1] = nn.Linear(self.features,num_classes)
    if self.model_name=='googlenet':
      self.model = models.googlenet(weights=None)
      self.features = self.model.fc.in_features 
      self.model.fc = nn.Linear(self.features,num_classes)
    if self.model_name=='mnasnet0_5':
      self.model = models.mnasnet0_5(weights=None)
      self.features = self.model.classifier[-1].in_features 
      self.model.classifier[-1] = nn.Linear(self.features,num_classes)
    if self.model_name=='regnet_y_400mf':
      self.model = models.regnet_y_400mf(weights=None)
      self.features = self.model.fc.in_features 
      self.model.fc = nn.Linear(self.features,num_classes)
    if self.model_name=='resnext50_32x4d':
      self.model = models.resnext50_32x4d(weights=None)
      self.features = self.model.fc.in_features 
      self.model.fc = nn.Linear(self.features,num_classes)
    if self.model_name=='shufflenet_v2_x0_5':
      self.model = models.shufflenet_v2_x0_5(weights=None)
      self.features = self.model.fc.in_features 
      self.model.fc = nn.Linear(self.features,num_classes)
    if self.model_name=='squeezenet1_0':
      self.model = models.squeezenet1_0(weights=None)
      self.features = self.model.classifier[-3].in_channels
      self.model.classifier[-3] = nn.Conv2d(self.features, num_classes, kernel_size=(1, 1), stride=(1, 1))
    if self.model_name=='vgg11':
      self.model = models.vgg11(weights=None)
      self.features = self.model.classifier[-1].in_features 
      self.model.classifier[-1] = nn.Linear(self.features,num_classes)
    if self.model_name=='wide_resnet50_2':
      self.model = models.wide_resnet50_2(weights=None)
      self.features = self.model.fc.in_features 
      self.model.fc = nn.Linear(self.features,num_classes)
    if self.model_name=='swin_t':
      self.model = models.swin_t(weights=None)
      self.features = self.model.head.in_features
      self.model.head = nn.Linear(self.features,num_classes)
    if self.model_name=='densenet_121':
      self.model = models.densenet121(weights=None)
      self.features = self.model.classifier.in_features
      self.model.classifier = nn.Linear(self.features,num_classes)
    if self.model_name=='efficientnet_b0':
      self.model = models.efficientnet.efficientnet_b0(weights=None)
      self.features = self.model.classifier[-1].in_features
      self.model.classifier[-1] = nn.Linear(self.features,num_classes)
    if self.model_name=='mobilenet_v3_large':
      self.model = models.mobilenet_v3_large(weights=None)
      self.features = self.model.classifier[-1].in_features
      self.model.classifier[-1] = nn.Linear(self.features,num_classes)
    self.loss = nn.CrossEntropyLoss()

  def forward(self, x):
    return self.model(x)

  def training_step(self, batch, batch_no):
    x, y = batch
    if self.model_name=='googlenet':
      logits = self(x).logits
    else:
      logits = self(x)
    loss = self.loss(logits, y)
    return loss

  def configure_optimizers(self):
    return torch.optim.Adam(self.parameters(),lr=0.001)

mnist_train_dataset = datasets.MNIST(root="./data", train=True, download=True, transform=transform_mnist)
mnist_test_dataset = datasets.MNIST(root="./data", train=False, download=True, transform=transform_mnist)
cmnist_train_dataset = datasets.MNIST(root="./data", train=True, download=True, transform=transform_cmnist)
cmnist_test_dataset = datasets.MNIST(root="./data", train=False, download=True, transform=transform_cmnist)
svhn_train_dataset = datasets.SVHN(root="./data", split="train", download=True, transform=transform_svhn)
svhn_test_dataset = datasets.SVHN(root="./data", split="test", download=True, transform=transform_svhn)

mnist_train_loader = random_pick_mnist(1000, mnist_train_dataset,4)
mnist_test_loader = random_pick_mnist(500, mnist_test_dataset,1)
cmnist_train_loader = random_pick_mnist(1000, cmnist_train_dataset,4)
cmnist_test_loader = random_pick_mnist(500, cmnist_test_dataset,1)
svhn_train_loader = random_pick_svhn(1000, svhn_train_dataset,4)
svhn_test_loader = random_pick_svhn(500, svhn_test_dataset,1)
client_datasets = [(mnist_train_loader,mnist_test_loader),(cmnist_train_loader,cmnist_test_loader),(svhn_train_loader,svhn_test_loader)]

train_datasets = [mnist_train_loader.dataset, cmnist_train_loader.dataset, svhn_train_loader.dataset]
test_datasets = [mnist_test_loader.dataset, cmnist_test_loader.dataset, svhn_test_loader.dataset]

train_dataset = ConcatDataset(train_datasets)
test_dataset = ConcatDataset(test_datasets)

class IterableConcatDataset(IterableDataset):
    def __init__(self, datasets):
        self.datasets = datasets

    def __iter__(self):
        for dataset in self.datasets:
            yield from iter(dataset)

train_iterable_dataset = IterableConcatDataset(train_datasets)
test_iterable_dataset = IterableConcatDataset(test_datasets)

train_dataloader = DataLoader(train_iterable_dataset, batch_size=4)
test_dataloader = DataLoader(test_iterable_dataset, batch_size=1)

"""# Using For-Loop Method

##GoogleNet
"""

model, tmodels = federated_learning(client_datasets, num_epochs=20, name='googlenet')

detect(mnist_test_loader,model.eval())
detect(cmnist_test_loader,model.eval())
detect(svhn_test_loader,model.eval())

detect(mnist_test_loader,tmodels[0].eval())
detect(cmnist_test_loader,tmodels[1].eval())
detect(svhn_test_loader,tmodels[2].eval())
del model, tmodels

"""## MobileNet_v3_Large"""

model, tmodels = federated_learning(client_datasets, num_epochs=20, name='mobilenet_v3_large')
detect(mnist_test_loader,model.eval())
detect(cmnist_test_loader,model.eval())
detect(svhn_test_loader,model.eval())

detect(mnist_test_loader,tmodels[0].eval())
detect(cmnist_test_loader,tmodels[1].eval())
detect(svhn_test_loader,tmodels[2].eval())
del model, tmodels

"""## SqueezeNet1_0"""

model, tmodels = federated_learning(client_datasets, num_epochs=20, name='squeezenet1_0')
detect(mnist_test_loader,model.eval())
detect(cmnist_test_loader,model.eval())
detect(svhn_test_loader,model.eval())

detect(mnist_test_loader,tmodels[0].eval())
detect(cmnist_test_loader,tmodels[1].eval())
detect(svhn_test_loader,tmodels[2].eval())
del model, tmodels

"""## Shufflenet_v2_x0_5"""

model, tmodels = federated_learning(client_datasets, num_epochs=20, name='shufflenet_v2_x0_5')
detect(mnist_test_loader,model.eval())
detect(cmnist_test_loader,model.eval())
detect(svhn_test_loader,model.eval())

detect(mnist_test_loader,tmodels[0].eval())
detect(cmnist_test_loader,tmodels[1].eval())
detect(svhn_test_loader,tmodels[2].eval())
del model, tmodels

"""## Convnext_tiny"""

model, tmodels = federated_learning(client_datasets, num_epochs=20, name='convnext_tiny')
detect(mnist_test_loader,model.eval())
detect(cmnist_test_loader,model.eval())
detect(svhn_test_loader,model.eval())

detect(mnist_test_loader,tmodels[0].eval())
detect(cmnist_test_loader,tmodels[1].eval())
detect(svhn_test_loader,tmodels[2].eval())
del model, tmodels

"""## Regnet_y_400mf"""

model, tmodels = federated_learning(client_datasets, num_epochs=20, name='regnet_y_400mf')
detect(mnist_test_loader,model.eval())
detect(cmnist_test_loader,model.eval())
detect(svhn_test_loader,model.eval())

detect(mnist_test_loader,tmodels[0].eval())
detect(cmnist_test_loader,tmodels[1].eval())
detect(svhn_test_loader,tmodels[2].eval())
del model, tmodels

"""## Resnet_34"""

model, tmodels = federated_learning(client_datasets, num_epochs=20, name='resnet_34')
detect(mnist_test_loader,model.eval())
detect(cmnist_test_loader,model.eval())
detect(svhn_test_loader,model.eval())

detect(mnist_test_loader,tmodels[0].eval())
detect(cmnist_test_loader,tmodels[1].eval())
detect(svhn_test_loader,tmodels[2].eval())
del model, tmodels

"""#Using Normal Method

## mobileNet_v3_large
"""

client_model_1 = Model('mobilenet_v3_large',10)
trainer = pl.Trainer(accelerator='gpu', max_epochs=10, default_root_dir='/content/drive/MyDrive/DAI_Assignment_3/Normal_Method/mobilenet_v3_large_1', benchmark=True)
trainer.fit(client_model_1, mnist_train_loader)
del client_model_1, trainer

client_model_2 = Model('mobilenet_v3_large',10)
trainer = pl.Trainer(accelerator='gpu', max_epochs=10, default_root_dir='/content/drive/MyDrive/DAI_Assignment_3/Normal_Method/mobilenet_v3_large_2', benchmark=True)
trainer.fit(client_model_2, cmnist_train_loader)
del client_model_2, trainer

client_model_3 = Model('mobilenet_v3_large',10)
trainer = pl.Trainer(accelerator='gpu', max_epochs=10, default_root_dir='/content/drive/MyDrive/DAI_Assignment_3/Normal_Method/mobilenet_v3_large_3', benchmark=True)
trainer.fit(client_model_3, cmnist_train_loader)
del client_model_3, trainer

client_model_1 = model_return('/content/drive/MyDrive/DAI_Assignment_3/Normal_Method/mobilenet_v3_large_1/lightning_logs/version_0/checkpoints/epoch=9-step=25000.ckpt', Model('mobilenet_v3_large',10))
client_model_2 = model_return('/content/drive/MyDrive/DAI_Assignment_3/Normal_Method/mobilenet_v3_large_2/lightning_logs/version_0/checkpoints/epoch=9-step=25000.ckpt', Model('mobilenet_v3_large',10))
client_model_3 = model_return('/content/drive/MyDrive/DAI_Assignment_3/Normal_Method/mobilenet_v3_large_3/lightning_logs/version_0/checkpoints/epoch=9-step=25000.ckpt', Model('mobilenet_v3_large',10))

detect(mnist_test_loader, client_model_1)

detect(mnist_test_loader, client_model_2)

detect(svhn_test_loader, client_model_3)

server_model = Model('mobilenet_v3_large',10).eval().cuda()
for name, param in server_model.named_parameters():
  param.data = torch.zeros_like(param.data)
  for client_model in [client_model_1,client_model_2,client_model_3]:
    param.data += client_model.state_dict()[name] / 3

detect(mnist_test_loader, server_model)

detect(cmnist_test_loader, server_model)

detect(svhn_test_loader, server_model)

client_models = [client_model_1, client_model_2, client_model_3]
server_model = Model('mobilenet_v3_large',10).eval().cuda()
server_model = server_aggregate(server_model, client_models)
detect(mnist_test_loader, server_model)

del client_model_1, client_model_2, client_model_3, server_model

combined_model_1 = Model('mobilenet_v3_large',10)
trainer = pl.Trainer(accelerator='gpu', max_epochs=10, default_root_dir='/content/drive/MyDrive/DAI_Assignment_3/Normal_Method/mobilenet_v3_large_combined', benchmark=True)
trainer.fit(combined_model_1, train_dataloader)
detect(test_dataloader, combined_model_1.eval())
del combined_model_1, trainer

"""## swin_t"""

client_model_1 = Model('swin_t',10)
trainer = pl.Trainer(accelerator='gpu', max_epochs=10, default_root_dir='/content/drive/MyDrive/DAI_Assignment_3/Normal_Method/swin_t_1', benchmark=True)
trainer.fit(client_model_1, mnist_train_loader)
del client_model_1, trainer

client_model_2 = Model('swin_t',10)
trainer = pl.Trainer(accelerator='gpu', max_epochs=10, default_root_dir='/content/drive/MyDrive/DAI_Assignment_3/Normal_Method/swin_t_2', benchmark=True)
trainer.fit(client_model_2, cmnist_train_loader)
del client_model_2, trainer

client_model_3 = Model('swin_t',10)
trainer = pl.Trainer(accelerator='gpu', max_epochs=10, default_root_dir='/content/drive/MyDrive/DAI_Assignment_3/Normal_Method/swin_t_3', benchmark=True)
trainer.fit(client_model_3, cmnist_train_loader)
del client_model_3, trainer

client_model_1 = model_return('/content/drive/MyDrive/DAI_Assignment_3/Normal_Method/swin_t_1/lightning_logs/version_0/checkpoints/epoch=9-step=25000.ckpt', Model('swin_t',10))
client_model_2 = model_return('/content/drive/MyDrive/DAI_Assignment_3/Normal_Method/swin_t_2/lightning_logs/version_0/checkpoints/epoch=9-step=25000.ckpt', Model('swin_t',10))
client_model_3 = model_return('/content/drive/MyDrive/DAI_Assignment_3/Normal_Method/swin_t_3/lightning_logs/version_0/checkpoints/epoch=9-step=25000.ckpt', Model('swin_t',10))

detect(mnist_test_loader, client_model_1)

detect(mnist_test_loader, client_model_2)

detect(svhn_test_loader, client_model_3)

server_model = Model('swin_t',10).eval().cuda()
for name, param in server_model.named_parameters():
  param.data = torch.zeros_like(param.data)
  for client_model in [client_model_1,client_model_2,client_model_3]:
    param.data += client_model.state_dict()[name] / 3

detect(mnist_test_loader, server_model)

detect(cmnist_test_loader, server_model)

detect(svhn_test_loader, server_model)

del client_model_1, client_model_2, client_model_3, server_model

combined_model_1 = Model('swin_t',10)
trainer = pl.Trainer(accelerator='gpu', max_epochs=10, default_root_dir='/content/drive/MyDrive/DAI_Assignment_3/Normal_Method/swin_t_combined', benchmark=True)
trainer.fit(combined_model_1, train_dataloader)
detect(test_dataloader, combined_model_1.eval())
del combined_model_1, trainer

"""## densenet_121"""

client_model_1 = Model('densenet_121',10)
trainer = pl.Trainer(accelerator='gpu', max_epochs=10, default_root_dir='/content/drive/MyDrive/DAI_Assignment_3/Normal_Method/densenet_121_1', benchmark=True)
trainer.fit(client_model_1, mnist_train_loader)
del client_model_1, trainer

client_model_2 = Model('densenet_121',10)
trainer = pl.Trainer(accelerator='gpu', max_epochs=10, default_root_dir='/content/drive/MyDrive/DAI_Assignment_3/Normal_Method/densenet_121_2', benchmark=True)
trainer.fit(client_model_2, cmnist_train_loader)
del client_model_2, trainer

client_model_3 = Model('densenet_121',10)
trainer = pl.Trainer(accelerator='gpu', max_epochs=10, default_root_dir='/content/drive/MyDrive/DAI_Assignment_3/Normal_Method/densenet_121_3', benchmark=True)
trainer.fit(client_model_3, cmnist_train_loader)
del client_model_3, trainer

client_model_1 = model_return('/content/drive/MyDrive/DAI_Assignment_3/Normal_Method/densenet_121_1/lightning_logs/version_0/checkpoints/epoch=9-step=25000.ckpt', Model('densenet_121',10))
client_model_2 = model_return('/content/drive/MyDrive/DAI_Assignment_3/Normal_Method/densenet_121_2/lightning_logs/version_0/checkpoints/epoch=9-step=25000.ckpt', Model('densenet_121',10))
client_model_3 = model_return('/content/drive/MyDrive/DAI_Assignment_3/Normal_Method/densenet_121_3/lightning_logs/version_1/checkpoints/epoch=9-step=25000.ckpt', Model('densenet_121',10))

detect(mnist_test_loader, client_model_1)

detect(mnist_test_loader, client_model_2)

detect(svhn_test_loader, client_model_3)

server_model = Model('densenet_121',10).eval().cuda()
for name, param in server_model.named_parameters():
  param.data = torch.zeros_like(param.data)
  for client_model in [client_model_1,client_model_2,client_model_3]:
    param.data += client_model.state_dict()[name] / 3

detect(mnist_test_loader, server_model)

detect(cmnist_test_loader, server_model)

detect(svhn_test_loader, server_model)

del client_model_1, client_model_2, client_model_3, server_model